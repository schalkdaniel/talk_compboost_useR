\documentclass[10pt]{beamer}

%% include header:
\input{./header}

%% include template:
\input{./templates/metropolis_cert}



%% kableExtra stuff:
%% ----------------------------------------

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}


\definecolor{metropolis_orange}{RGB}{225, 127, 0}


%% Title:
%% ----------------------------------------

\title{compboost}
\subtitle{Fast and Flexible Component-Wise Boosting Framework}
% \date{\today}
\date{July 12, 2019}
\author{\textbf{Daniel Schalk}, Janek Thomas, and Bernd Bischl}
\institute{LMU Munich\\Working Group Computational Statistics}

%% Wrap Shaded around Shunk to have a nices R output:
%% --------------------------------------------------

\let\Oldkframe\kframe
\let\endOldkframe\endkframe

\renewenvironment{kframe}
 {\scriptsize\definecolor{shadecolor}{RGB}{240,240,240}\begin{Shaded}\Oldkframe}
 {\endOldkframe\end{Shaded}\normalsize}

%% Prevent code from printing over margin:
%% --------------------------------------------------

<<echo=FALSE>>=
options(width=60)
@

%% Content:
%% ----------------------------------------

\begin{document}







%% SETUP Chunk:
%% ----------------------------------------

<<include=FALSE>>=
library(knitr)
opts_chunk$set(
  concordance = TRUE
)


library(compboost)
library(kableExtra)
library(ggplot2)
library(dplyr)
library(ggpubr)





slide_theme = theme_pubr(base_size = 14, legend = "right") +
  theme(
    plot.background = element_rect(fill = "transparent", color = NA),
    legend.background = element_rect(fill = "transparent", color = NA),
    legend.box.background = element_rect(fill = "transparent", color = NA)
  )


n_sim = 200
n_noise_features = 200L

set.seed(314159)

country = sample(x = c("Austria", "Seychelles", "Germany", "Czechia", "Australia", "USA"),
  size = n_sim, replace = TRUE)

country_biases = c(Austria = 106, Seychelles = 111, Germany = 104, Czechia = 140,
  Australia = 71, USA = 74)

age = sample(x = 16:70, size = n_sim, replace = TRUE)
ageFun = function (age) { return ((10 - 0.1 * (age - 16)^2 + 0.002 * (age - 16)^3) / 1) }
contr_age = ageFun(age)

# plot(age, contr_age)

gender = sample(x = c("m", "f"), size = n_sim, replace = TRUE)
gender_biases = c(m = 5, f = -2)

beer_consumption = country_biases[country] + gender_biases[gender] + contr_age + rnorm(n = n_sim, mean = 0, sd = 2)
beer_consumption = ifelse(beer_consumption < 0, 0, beer_consumption)

beer_data = data.frame(
  beer_consumption = round(beer_consumption / 0.5) * 0.5,
  # beer_consumption_cups = round(beer_consumption / 0.5),
  gender = gender,
  country = country,
  age = age,
  weight = runif(n = n_sim, min = 60, max = 120),
  height = runif(n = n_sim, min = 156, max = 204)
)

for (i in seq_len(n_noise_features)) {
  set.seed(i * 3)
  beer_data[[paste0("app_usage", i)]] = runif(n = n_sim)
}

# head(beer_data)
# plot(density(beer_data$beer_consumption))
@

\maketitle



\section{Use-Case}

\begin{frame}{The Situation}

\begin{itemize}
  \item
    We own a small booth at the city center that sells beer.

  \item
    As we are very interested in our customers' health, we only sell to customers who we expect to drink less than 110 liters per year.

  \item
    To estimate how much a customer drinks, we have collected data from 200 customers in recent years.

  \item
    These data include the beer consumption (in liter), age, sex, country of origin, weight, body size, and 200 characteristics gained from app usage (that have absolutely no influence).
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Overview of the Data}

<<echo=FALSE>>=
df_beer = beer_data[1:10, 1:7]
df_beer$`...` = "..."
df_beer[[names(beer_data)[ncol(beer_data)]]] = beer_data[1:10, ncol(beer_data)]
kable(df_beer, "latex", booktabs = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
@

\end{frame}


\begin{frame}{Our Goals}

With these data we want to answer the following questions:

\begin{itemize}
  \item
    Which of the customers' characteristics are important to be able to determine the consumption?
  \item
    How does the effect of important features look like?
  \item
    How does the model behave on unseen data?
\end{itemize}

\end{frame}




\section{What is Component-Wise Boosting?}

\begin{frame}{General Idea}

\begin{center}
\includegraphics[width=0.8\textwidth]{./images/cboost_gif_norisk.png}
\end{center}

\vspace{-1cm}

\begin{itemize}

  \item
    Sequential fitting of the base-learner $b_1, b_2, b_3$ on the error / pseudo-residuals of the current ensemble.

  \item
    The base-learner with the best fit on the error (measured as mean squared error) is added to the ensemble.

  \item
    Results in a weighted sum / additive model over base-learners.

\end{itemize}

\end{frame}

\begin{frame}{Advantages of Component-Wise Boosting}

  \begin{itemize}

    \item
      Inherent (unbiased) feature selection.

    \item
      Resulting model is sparse since important effects are selected first and therefore it is able to learn in high-dimensional feature spaces ($p \gg n$).

    \item
      Parameters are updated iteratively. Therefore, the whole trace of how the model evolves is available. \vspace{0.3cm}

  \end{itemize}

\end{frame}

\begin{frame}{Base-Learner Paths}

<<echo=FALSE, results="hide", messages=FALSE, warnings=FALSE>>=
set.seed(618)
cboost = boostSplines(data = beer_data, target = "beer_consumption",
  loss = LossAbsolute$new(), learning_rate = 0.1, iterations = 2000L,
  penalty = 10, oob_fraction = 0.3, trace = 0)

bl = as.factor(cboost$getSelectedBaselearner())
bl_table = table(bl) / length(bl)

# df_plot = data.frame(iters = seq_along(bl), blearner = bl, bl_cumsum = -diff(cboost$model$getRiskVector()))
df_plot = data.frame(iters = seq_along(bl), blearner = bl, bl_cumsum = 1)


n_legend = 5L
top_labs = as.factor(names(sort(bl_table, decreasing = TRUE)))[seq_len(n_legend)]

df_plot = df_plot %>%
  group_by(blearner) %>%
  mutate(bl_cumsum = cumsum(bl_cumsum) / length(bl)) %>%
  arrange(desc(bl_cumsum))

df_plot_top = df_plot %>%
  filter(blearner %in% top_labs)

df_plot_nottop = df_plot %>%
  filter(! blearner %in% top_labs)
@

<<echo=FALSE, fig.width=7.7, fig.height=3.75, out.width="\\textwidth", fig.align="center">>=
library(ggrepel)

ggplot() +
  geom_line(data = df_plot_top, aes(x = iters, y = bl_cumsum, color = blearner), show.legend = FALSE) +
  geom_line(data = df_plot_nottop, aes(x = iters, y = bl_cumsum, group = blearner), alpha = 0.2, show.legend = FALSE) +
  geom_label_repel(data = df_plot_top %>% filter(iters == max(iters)),
    aes(x = iters, y = bl_cumsum, label = round(bl_cumsum, 4), fill = blearner),
    colour = "white", fontface = "bold", show.legend = TRUE) +
  xlab("Iteration") +
  ylab("Relative Frequency \nof Included Base-Learner") +
  scale_fill_discrete(name = paste0("Top ", n_legend, " Base-Learner")) +
  guides(color = FALSE) +
  slide_theme
@

\end{frame}


\section{About Compboost}

\begin{frame}{Current Standard}

Most popular package for model-based boosting is \texttt{mboost}:

\begin{itemize}

  \item
    Large number of available base-learner and losses.

  \item
    Extended to more complex problems:
    \begin{itemize}
      \item Functional data
      \item GAMLSS models
      \item Survival analysis
    \end{itemize}
  \item
    Extendible with custom base-learner and losses.

\end{itemize}

\textbf{So, why another boosting implementation?}

\begin{itemize}

  \item
    Main parts of \texttt{mboost} are written in \texttt{R} and gets slow for large datasets.

  \item
    Complex implementation:
    \begin{itemize}
      \item Nested scopes
      \item Mixture of different \texttt{R} class systems
    \end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{About Compboost}


Fast and flexible framework for model-based boosting:

\begin{itemize}

  \item
    With \texttt{mboost} as standard, we want to keep the modular principle of defining custom base-learner and losses.

  \item
    Completely written in \texttt{C++} and exposed by \texttt{Rcpp} to obtain high performance and full memory control.

  \item
    \texttt{R} API is written in \texttt{R6} to provide convenient wrapper.

  \item
    Major parts of the \texttt{compboost} functionality are unit tested against \texttt{mboost} to ensure correctness.

\end{itemize}

\end{frame}



\section{Small Demonstration}

\begin{frame}[fragile]{Starting With Convenience Wrapper}

\texttt{boostLinear()} and \texttt{boostSplines()} automatically adds univariate linear models or a GAM for all features.

<<warning=FALSE>>=
set.seed(618)
cboost = boostSplines(data = beer_data, target = "beer_consumption",
  loss = LossAbsolute$new(), learning_rate = 0.1, iterations = 5000L,
  penalty = 10, oob_fraction = 0.3, trace = 2500L)
@

\end{frame}

\begin{frame}[fragile]{Visualizing the Results}

<<eval=FALSE>>=
gg1 = cboost$plotInbagVsOobRisk()
gg2 = cboost$plotFeatureImportance()
@

<<echo=FALSE, fig.width=10, fig.height=5, out.width="\\textwidth">>=
gg1 = cboost$plotInbagVsOobRisk() + slide_theme
gg2 = cboost$plotFeatureImportance() + slide_theme

gridExtra::grid.arrange(gg1, gg2, ncol = 2L)
@

\end{frame}

\begin{frame}[fragile]{Visualizing the Results}

<<eval=FALSE>>=
cboost$train(2000L)

gg1 = cboost$plotFeatureImportance()
gg2 = cboost$plot("age_spline", iters = c(50, 100, 500, 1000, 2000, 4000))
@


<<echo=FALSE, fig.width=10, fig.height=5, out.width="\\textwidth">>=
cboost$train(2000L)

gg1 = cboost$plotFeatureImportance() + slide_theme
gg2 = cboost$plot("age_spline", iters = c(50, 100, 500, 1000, 2000, 4000)) + slide_theme

gridExtra::grid.arrange(gg1, gg2, ncol = 2L)
@

\end{frame}

\begin{frame}[fragile]{Using the R6 Interface}

<<>>=
cboost = Compboost$new(data = beer_data, target = "beer_consumption",
  loss = LossQuantile$new(0.9), learning_rate = 0.1, oob_fraction = 0.3)

cboost$addBaselearner("age", "spline", BaselearnerPSpline)
cboost$addBaselearner("country", "category", BaselearnerPolynomial)

cboost$addLogger(logger = LoggerTime, use_as_stopper = TRUE, logger_id = "time",
  max_time = 2e5, time_unit = "microseconds")

cboost$train(10000, trace = 500)
@

\end{frame}

\begin{frame}{Overview of the Functionality}

\begin{itemize}

  \item
    \textbf{Base-learner:} \texttt{BaselearnerPolynomial}, \texttt{BaselearnerSpline}, \texttt{BaselearnerCustom}, and \texttt{BaselearnerCustomCpp}

  \item
    \textbf{Loss functions:} \texttt{LossQuadratic}, \texttt{LossAbsolute}, \texttt{LossQuantile}, \texttt{LossHuber}, \texttt{LossBinomial}, \texttt{LossCustom}, and \texttt{LossCustomCpp}

  \item
    \textbf{Logger/Stopper:} \texttt{LoggerIteration}, \texttt{LoggerInbagRisk}, \texttt{LoggerOobRisk}, and \texttt{LoggerTime} \\
    \begin{itemize}
      \item[$\rightarrow$]
        Performance-based early stopping can be applied using the \texttt{LoggerOobRisk} and specifying the relative improvement that should be reached (e.g. 0 for stopping when out of bag risk starts to increase).
    \end{itemize}

\end{itemize}

\end{frame}

\section{Performance Considerations}

\begin{frame}{Performance Considerations}


\begin{itemize}

  \item
    Optimizer are parallelized via openmp:\vspace{0.3cm}

<<echo=FALSE, fig.width=5, fig.height=2.5, out.width="0.7\\textwidth", fig.align="center">>=
# ## Code for figure:
# cores = c(1, 2, 4, 8, 16)
# reps = 10L
# df_res = data.frame(runtime = numeric(length = reps * length(cores)), cores = as.factor(rep(cores, each = reps)))
# for (i in seq_along(cores)) {
#   for (j in seq_len(reps)) {

#     time = proc.time()
#     temp = boostSplines(data = beer_data, target = "beer_consumption",
#       loss = LossAbsolute$new(), learning_rate = 0.1, iterations = 5000L,
#       penalty = 10, oob_fraction = 0.3, trace = -1, optimizer = OptimizerCoordinateDescent$new(cores[i]))
#     time = proc.time() - time

#     df_res[j + (i - 1) * reps, 1] = time[3]
#   }
# }
# save(list = "df_res", file = "images/cboost_runtime.rds")

load("images/cboost_runtime.rds")

df_res %>%
  mutate(runtime_relative = median(df_res[df_res$cores == 1, "runtime"]) / runtime) %>%
  ggplot(aes(x = cores, y = runtime_relative)) +
    geom_boxplot(fill = "#E17F00") +
    xlab("Number of Threads") +
    ylab("Speedup\n(compared to one thread)") +
    transparent_theme
@

  \item
    Take advantage of the matrix structure to speed up the algorithm by reducing the number of repetitive or too expensive calculations.

  \item
    Matrices are stored (if possible) as a sparse matrix.

\end{itemize}

\end{frame}

\begin{frame}{Small Comparison With Mboost}

\begin{itemize}

  \item
    Runtime (in minutes):\vspace{-0.3cm}
    \begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
      \begin{tabular}{c|c|c|c}
        \textbf{nrows / ncols} & \parbox{3cm}{\centering \textbf{mboost}} & \parbox{3cm}{\centering \textbf{compboost}} & \parbox{3cm}{\centering \textbf{compboost} \\ \textbf{(16 threads)}} \\
        \midrule
        20000 / 200  &  21.10 (1) &  10.47 (2.02) & 0.95 (22.21) \\
        20000 / 2000 & 216.70 (1) &  83.95 (2.58) & 8.15 (26.59)
      \end{tabular}
    }
    \end{table}

  \item
    Memory (in GB):\vspace{-0.3cm}
    \begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
      \begin{tabular}{c|c|c|c}
        \textbf{nrows / ncols} & \parbox{3cm}{\centering \textbf{mboost}} & \parbox{3cm}{\centering \textbf{compboost}} & \parbox{3cm}{\centering \textbf{compboost} \\ \textbf{(16 threads)}} \\
        \midrule
        20000 / 200  & 1.04 (1) & 0.28 (3.71) & 0.30 (3.47) \\
        20000 / 2000 & 8.70 (1) & 2.60 (3.35) & 2.98 (2.92)
      \end{tabular}
    }
    \end{table}

\end{itemize}

(Comparison was made by just using spline base-learner with 20 knots and and 5000 iterations. The numbers in the brackets are the relative values compared to \texttt{mboost}.)


% \begin{table}[H]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c||c|c||c|c||c|c}
%  & \multicolumn{2}{c||}{\textbf{mboost}} & \multicolumn{2}{c||}{\textbf{compboost}} & \multicolumn{2}{c}{\textbf{compboost (16 threads)}} \\
%  \textbf{nrows / ncols} & \textbf{Memory Usage} & \textbf{Runtime} & \textbf{Memory Usage} & \textbf{Runtime} & \textbf{Memory Usage} & \textbf{Runtime} \\
% \midrule
% 20000 / 200  & 1.04 (1) &  21.1 (1) & 0.28 (3.7) & 10.47 (2) & 0.3 (3.5)  & 0.95 (22) \\
% 20000 / 2000 & 8.70 (1) & 216.7 (1) & 2.60 (3.3) & 83.95 (2.5) & 2.98 (3) & 8.15 (25)
% \end{tabular}
% }
% \end{table}

\end{frame}


\section{What's Next?}

\begin{frame}{What's Next?}

\begin{itemize}
  \item
    Research on computational aspects of the algorithm:
    \begin{itemize}
      \item
        More stable base-learner selection process via resampling
      \item
        Base-learner selection for arbitrary performance measures
      \item
        Smarter and faster optimizers
    \end{itemize}

  \item
    Greater functionality:
    \begin{itemize}
      \item
        Functional data structures and loss functions
      \item
        Unbiased feature selection
      \item
        Effect decomposition into constant, linear, and non-linear
    \end{itemize}

    \item
      Reducing the memory load by applying binning on numerical features.

    \item
      Adding hyperparameter tuning by providing a \texttt{mlr} (\texttt{mlr3}) learner API.

    \item
      Exposing \texttt{C++} classes to python.
\end{itemize}

\end{frame}

% \addtocounter{framenumber}{-1}

\begin{frame}

\begin{itemize}

  \item
    Slides are available at:
    \vspace{0.2cm}
    \begin{center}
      \url{www.github.com/schalkdaniel/talk_compboost_useR}
    \end{center}
    \vspace{0.3cm}

  \item
    Actively developed on GitHub:
    \vspace{0.2cm}
    \begin{center}
      \url{www.github.com/schalkdaniel/compboost}
    \end{center}
    \vspace{0.3cm}

  \item
    Project page:
    \vspace{0.2cm}
    \begin{center}
      \url{www.compboost.org}
    \end{center}
    \vspace{0.3cm}

  \item
    JOSS DOI:
    \vspace{0.2cm}
    \begin{center}
      \href{https://joss.theoj.org/papers/94cfdbbfdfc8796c5bdb1a74ee59fcda}{\textcolor{metropolis_orange}{10.21105/joss.00967}}
    \end{center}

\end{itemize}

\end{frame}


\end{document}