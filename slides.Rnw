\documentclass[10pt]{beamer}

%% include header:
\input{./header}

%% include template:
\input{./templates/metropolis_cert}



%% kableExtra stuff:
%% ----------------------------------------

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}


\definecolor{metropolis_orange}{RGB}{225, 127, 0}


%% Title:
%% ----------------------------------------

\title{compboost}
\subtitle{Fast and Flexible Component-Wise Boosting Framework}
% \date{\today}
\date{July 12, 2019}
\author{Daniel Schalk}
\institute{LMU Munich\\Working Group Computational Statistics}

%% Wrap Shaded around Shunk to have a nices R output:
%% --------------------------------------------------

\let\Oldkframe\kframe
\let\endOldkframe\endkframe

\renewenvironment{kframe}
 {\scriptsize\definecolor{shadecolor}{RGB}{240,240,240}\begin{Shaded}\Oldkframe}
 {\endOldkframe\end{Shaded}\normalsize}

%% Prevent code from printing over margin:
%% --------------------------------------------------

<<echo=FALSE>>=
options(width=60)
@

%% Content:
%% ----------------------------------------

\begin{document}







%% SETUP Chunk:
%% ----------------------------------------

<<include=FALSE>>=
library(knitr)
opts_chunk$set(
  concordance = TRUE
)


library(compboost)
library(kableExtra)
library(ggplot2)



transparent_theme = theme(
  plot.background = element_rect(fill = "transparent", color = NA),
  legend.background = element_rect(fill = "transparent", color = NA),
  legend.box.background = element_rect(fill = "transparent", color = NA)
)



n_sim = 200
n_noise_features = 200L

set.seed(314159)

country = sample(x = c("Austria", "Seychelles", "Germany", "Czechia", "Australia", "USA"),
  size = n_sim, replace = TRUE)

country_biases = c(Austria = 106, Seychelles = 111, Germany = 104, Czechia = 140,
  Australia = 71, USA = 74)

age = sample(x = 16:70, size = n_sim, replace = TRUE)
ageFun = function (age) { return ((10 - 0.1 * (age - 16)^2 + 0.002 * (age - 16)^3) / 1) }
contr_age = ageFun(age)

# plot(age, contr_age)

gender = sample(x = c("m", "f"), size = n_sim, replace = TRUE)
gender_biases = c(m = 5, f = -2)

beer_consumption = country_biases[country] + gender_biases[gender] + contr_age + rnorm(n = n_sim, mean = 0, sd = 2)
beer_consumption = ifelse(beer_consumption < 0, 0, beer_consumption)

beer_data = data.frame(
  beer_consumption = round(beer_consumption / 0.5) * 0.5,
  # beer_consumption_cups = round(beer_consumption / 0.5),
  gender = gender,
  country = country,
  age = age,
  weight = runif(n = n_sim, min = 60, max = 120),
  height = runif(n = n_sim, min = 156, max = 204)
)

for (i in seq_len(n_noise_features)) {
  set.seed(i * 3)
  beer_data[[paste0("app_usage", i)]] = runif(n = n_sim)
}

# head(beer_data)
# plot(density(beer_data$beer_consumption))
@

\maketitle



\section{Use-Case}

\begin{frame}{The Situation}

\begin{itemize}
  \item
    We own a small booth at the city center that sells beer.

  \item
    As we are very interested in our customers' health, we only sell to customers who we expect to drink less than 110 liters per year.

  \item
    To estimate how much a customer drinks, we have collected data from 200 customers in recent years.

  \item
    These data include the beer consumption (in liter), age, sex, country of origin, weight, body size, and 200 characteristics gained from app usage (that have absolutely no influence).
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Overview of the Data}

<<echo=FALSE>>=
df_beer = beer_data[1:10, 1:7]
df_beer$`...` = "..."
df_beer[[names(beer_data)[ncol(beer_data)]]] = beer_data[1:10, ncol(beer_data)]
kable(df_beer, "latex", booktabs = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
@

\end{frame}


\begin{frame}{Our Goals}

With these data we want to answer the following questions:

\begin{itemize}
  \item
    Which of the customers' characteristics are important to be able to determine the consumption?
  \item
    How does the effect of important features look like?
  \item
    How does the model behave on unseen data?
\end{itemize}

\end{frame}




\section{What is Component-Wise Boosting?}

\begin{frame}{General Idea}

\begin{center}
\includegraphics[width=\textwidth]{./images/cboost_gif_norisk.png}
\end{center}

\end{frame}

\begin{frame}{Advantages of Component-Wise Boosting}

  \begin{itemize}

    \item
      Inherent (unbiased) feature selection.

    \item
      Resulting model is sparse since important effects are selected first and therefore it is able to learn in high-dimensional feature spaces ($p \gg n$).

    \item
      Parameters are updated iteratively. Therefore, the whole trace of how the model evolves is available.

  \end{itemize}

\end{frame}




\section{About Compboost}

\begin{frame}{About Compboost}


The \texttt{compboost} package is a fast and flexible framework for model-based boosting:

\begin{itemize}

  \item
    With \texttt{mboost} as standard, we want to keep the modular principle of defining custom base-learner and losses.

  \item
    Completely written in \texttt{C++} and exposed by \texttt{Rcpp} to obtain high performance and full memory control.

  \item
    \texttt{R} API is written in \texttt{R6} to provide convenient wrapper.

  \item
    Major parts of the \texttt{compboost} functionality are unit tested against \texttt{mboost} to ensure correctness.

\end{itemize}

\end{frame}



\section{A Small Demonstration}

\begin{frame}[fragile]{Starting With Convenience Wrapper}

\texttt{boostLinear()} and \texttt{boostSplines()} automatically adds univariate linear models or a GAM for all features.

<<warning=FALSE>>=
set.seed(618)
cboost = boostSplines(data = beer_data, target = "beer_consumption",
  loss = LossAbsolute$new(), learning_rate = 0.1, iterations = 5000L,
  penalty = 10, oob_fraction = 0.3, trace = 2500L)
@

\end{frame}

\begin{frame}[fragile]{Visualizing the Results}

<<eval=FALSE>>=
gg1 = cboost$plotInbagVsOobRisk()
gg2 = cboost$plotFeatureImportance()
@

<<echo=FALSE, fig.width=10, fig.height=5, out.width="\\textwidth">>=
gg1 = cboost$plotInbagVsOobRisk() + transparent_theme
gg2 = cboost$plotFeatureImportance() + transparent_theme

gridExtra::grid.arrange(gg1, gg2, ncol = 2L)
@

\end{frame}

\begin{frame}[fragile]{Visualizing the Results}

<<eval=FALSE>>=
cboost$train(2000L)

gg1 = cboost$plotFeatureImportance()
gg2 = cboost$plot("age_spline", iters = c(50, 100, 500, 1000, 2000, 4000))
@


<<echo=FALSE, fig.width=10, fig.height=5, out.width="\\textwidth">>=
cboost$train(2000L)

gg1 = cboost$plotFeatureImportance() + transparent_theme
gg2 = cboost$plot("age_spline", iters = c(50, 100, 500, 1000, 2000, 4000)) + transparent_theme

gridExtra::grid.arrange(gg1, gg2, ncol = 2L)
@

\end{frame}

\begin{frame}[fragile]{Using the R6 Interface}

<<>>=
cboost = Compboost$new(data = beer_data, target = "beer_consumption",
  loss = LossQuantile$new(0.9), learning_rate = 0.1, oob_fraction = 0.3)

cboost$addBaselearner("age", "spline", BaselearnerPSpline)
cboost$addBaselearner("country", "category", BaselearnerPolynomial)

cboost$addLogger(logger = LoggerTime, use_as_stopper = TRUE, logger_id = "time",
  max_time = 2e5, time_unit = "microseconds")

cboost$train(10000, trace = 500)
@

\end{frame}

\begin{frame}{Overview of the Functionality}

\begin{itemize}

  \item
    \textbf{Base-learner:} \texttt{BaselearnerPolynomial}, \texttt{BaselearnerSpline}, \texttt{BaselearnerCustom}, and \texttt{BaselearnerCustomCpp}
    % \begin{itemize}
    %   \item Polynomial base-learner
    %   \item Spline base-learner
    %   \item Custom \texttt{R} or \texttt{C++} base-learner
    % \end{itemize}

  \item
    \textbf{Loss functions:} \texttt{LossQuadratic}, \texttt{LossAbsolute}, \texttt{LossQuantile}, \texttt{LossHuber}, \texttt{LossBinomial}, \texttt{LossCustom}, and \texttt{LossCustomCpp}

  \item
    \textbf{Logger/Stopper:} \texttt{LoggerIteration}, \texttt{LoggerInbagRisk}, \texttt{LoggerOobRisk}, and \texttt{LoggerTime}

\end{itemize}

\end{frame}

\section{Performance Considerations}

\begin{frame}{Performance Considerations}


\begin{itemize}

  \item
    Optimizer are parallelized via openmp:\vspace{0.3cm}

<<echo=FALSE, fig.width=5, fig.height=2.5, out.width="0.7\\textwidth", fig.align="center">>=
# ## Code for figure:
# cores = c(1, 2, 4, 8, 16)
# reps = 10L
# df_res = data.frame(runtime = numeric(length = reps * length(cores)), cores = as.factor(rep(cores, each = reps)))
# for (i in seq_along(cores)) {
#   for (j in seq_len(reps)) {

#     time = proc.time()
#     temp = boostSplines(data = beer_data, target = "beer_consumption",
#       loss = LossAbsolute$new(), learning_rate = 0.1, iterations = 5000L,
#       penalty = 10, oob_fraction = 0.3, trace = -1, optimizer = OptimizerCoordinateDescent$new(cores[i]))
#     time = proc.time() - time

#     df_res[j + (i - 1) * reps, 1] = time[3]
#   }
# }
# save(list = "df_res", file = "images/cboost_runtime.rds")

load("images/cboost_runtime.rds")

ggplot(data = df_res, aes(x = cores, y = runtime)) +
  geom_boxplot(fill = "#E17F00") +
  xlab("Number of Threads") +
  ylab("Runtime in Sec.") +
  transparent_theme
@

  \item
    Take advantage of the matrix structure to speed up the algorithm by reducing the number of repetitive or too expensive calculations.

  \item
    Matrices are stored (if possible) as a sparse matrix.

\end{itemize}

\end{frame}

\begin{frame}{Small Comparison With Mboost}

\begin{itemize}

  \item
    Runtime (numbers are given in minutes):\vspace{-0.3cm}
    \begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
      \begin{tabular}{c|c|c|c}
        \textbf{nrows / ncols} & \parbox{3cm}{\centering \textbf{mboost}} & \parbox{3cm}{\centering \textbf{compboost}} & \parbox{3cm}{\centering \textbf{compboost} \\ \textbf{(16 threads)}} \\
        \midrule
        20000 / 200  &  21.10 (1) &  10.47 (2.02) & 0.95 (22.21) \\
        20000 / 2000 & 216.70 (1) &  83.95 (2.58) & 8.15 (26.59)
      \end{tabular}
    }
    \end{table}

  \item
    Memory (numbers are given in GB):\vspace{-0.3cm}
    \begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
      \begin{tabular}{c|c|c|c}
        \textbf{nrows / ncols} & \parbox{3cm}{\centering \textbf{mboost}} & \parbox{3cm}{\centering \textbf{compboost}} & \parbox{3cm}{\centering \textbf{compboost} \\ \textbf{(16 threads)}} \\
        \midrule
        20000 / 200  & 1.04 (1) & 0.28 (3.71) & 0.30 (3.47) \\
        20000 / 2000 & 8.70 (1) & 2.60 (3.35) & 2.98 (2.92)
      \end{tabular}
    }
    \end{table}

\end{itemize}

(Comparison was made by just using spline base-learner with 20 knots and and 5000 iterations. The numbers in the brackets are the relative values compared to \texttt{mboost}.)


% \begin{table}[H]
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c||c|c||c|c||c|c}
%  & \multicolumn{2}{c||}{\textbf{mboost}} & \multicolumn{2}{c||}{\textbf{compboost}} & \multicolumn{2}{c}{\textbf{compboost (16 threads)}} \\
%  \textbf{nrows / ncols} & \textbf{Memory Usage} & \textbf{Runtime} & \textbf{Memory Usage} & \textbf{Runtime} & \textbf{Memory Usage} & \textbf{Runtime} \\
% \midrule
% 20000 / 200  & 1.04 (1) &  21.1 (1) & 0.28 (3.7) & 10.47 (2) & 0.3 (3.5)  & 0.95 (22) \\
% 20000 / 2000 & 8.70 (1) & 216.7 (1) & 2.60 (3.3) & 83.95 (2.5) & 2.98 (3) & 8.15 (25)
% \end{tabular}
% }
% \end{table}

\end{frame}


\section{What's Next?}

\begin{frame}{What's Next?}

\begin{itemize}
  \item
    Research on computational aspects of the algorithm:
    \begin{itemize}
      \item
        More stable base-learner selection process via resampling
      \item
        Base-learner selection for arbitrary performance measures
      \item
        Smarter and faster optimizers
    \end{itemize}

  \item
    Greater functionality:
    \begin{itemize}
      \item
        Functional data structures and loss functions
      \item
        Unbiased feature selection
      \item
        Effect decomposition into constant, linear, and non-linear
    \end{itemize}

    \item
      Reducing the memory load by applying binning on numerical features.

    \item
      Exposing \texttt{C++} classes to python.
\end{itemize}

\end{frame}

\addtocounter{framenumber}{-1}

\begin{frame}[plain, standout]
  Thanks for your attention!

  Questions?
\end{frame}

\begin{frame}[plain]

\begin{itemize}

  \item
    Actively developed on GitHub:
    \vspace{0.3cm}
    \begin{center}
      \url{www.github.com/schalkdaniel/compboost}
    \end{center}
    \vspace{0.5cm}

  \item
    Project page:
    \vspace{0.3cm}
    \begin{center}
      \url{www.compboost.org}
    \end{center}
    \vspace{0.5cm}

  \item
    JOSS DOI:
    \vspace{0.3cm}
    \begin{center}
      \href{https://joss.theoj.org/papers/94cfdbbfdfc8796c5bdb1a74ee59fcda}{\textcolor{metropolis_orange}{10.21105/joss.00967}}
    \end{center}

\end{itemize}

\end{frame}


\end{document}